# NSD S13 : Introduction of HLD

# **System Design Basics & Important Trade-offs**

## **Introduction: High-Level Design (HLD) Components**

At theÂ **infrastructure level**, a system consists of various components that help inÂ **handling, storing, processing, and delivering**Â data efficiently. These components are classified intoÂ **hardware and software**Â elements.

### **ğŸ”¹ Hardware Components**

1ï¸âƒ£Â **Servers**Â â€“ Machines that process requests and execute computations.

2ï¸âƒ£Â **Load Balancer (LB)**Â â€“ Distributes traffic across multiple servers to prevent overload.

3ï¸âƒ£Â **Database**Â â€“ Stores structured or unstructured data (SQL, NoSQL).

### **ğŸ”¹ Software Components**

4ï¸âƒ£Â **CDN (Content Delivery Network)**Â â€“ Stores cached copies of static content across multiple locations for faster access.

5ï¸âƒ£Â **Caching**Â â€“ Temporarily stores frequently accessed data to improve performance.

6ï¸âƒ£Â **Messaging Queue**Â â€“ A system that enables asynchronous communication between different services (e.g., Kafka, RabbitMQ).

7ï¸âƒ£Â **Rate Limiter**Â â€“ Prevents system abuse by restricting the number of requests a user can send within a time window.

These components workÂ **together**Â to createÂ **scalable, efficient, and reliable**Â software systems.

---

## **Important Terminologies & Trade-offs in System Design**

Let's explore some fundamentalÂ **concepts**Â in system design withÂ **real-world examples**.

---

### **CAP Theorem (Consistency, Availability, Partition Tolerance)**

CAP Theorem states that a distributed system can only achieve two out of three properties at a time:

- **Consistency (C)**Â â€“ All nodes return the most recent data.
- **Availability (A)**Â â€“ The system is always responsive.
- **Partition Tolerance (P)**Â â€“ The system continues working even when network failures occur.

**Example:**

- **CP (Consistency + Partition Tolerance):**Â MongoDB (sacrifices availability during network failures).
- **AP (Availability + Partition Tolerance):**Â Cassandra (sacrifices strong consistency to ensure uptime).

---

### **Vertical vs. Horizontal Scaling**

- Scaling means increasing system capacity to handle more requests.
- **Vertical Scaling:**Â Adding more power (CPU, RAM) to a single machine.
- **Horizontal Scaling:**Â Adding more machines (servers) to distribute the load.

**Example:**

- **Vertical Scaling:**Â Upgrading a server from 16GB RAM to 32GB.
- **Horizontal Scaling:**Â Adding 10 servers instead of a single powerful one (used by Google, Amazon).

---

### **Concurrency vs. Parallelism**

- Concurrency:Â Multiple tasks are processedÂ at the same timeÂ butÂ not necessarily executed simultaneously.
- **Parallelism:**Â Multiple tasks are executedÂ **at the exact same moment**Â using multiple CPU cores.

**Example:**

- **Concurrency:**Â A single-threaded Node.js server handling multiple API requests.
- **Parallelism:**Â Running a Machine Learning model on a multi-core processor.

---

### **Long Polling vs. WebSockets**

- Long Polling:Â Client repeatedly requests updates from the server.
- **WebSockets:**Â A continuous, open connection between client and server for real-time communication.

**Example:**

- **Long Polling:**Â Chat applications before WebSockets (e.g., AJAX-based chat).
- **WebSockets:**Â WhatsApp Web or Google Docs real-time collaboration.

---

### **Batch Processing vs. Stream Processing**

- Batch Processing:Â Data is processed in large chunks at scheduled times.
- **Stream Processing:**Â Data is processed in real-time as it arrives.

**Example:**

- **Batch Processing:**Â Payroll processing (salary calculated at the end of the month).
- **Stream Processing:**Â Fraud detection in banking (transactions analyzed instantly).

---

### **Stateful vs. Stateless Design**

- Stateful:Â The server remembers past interactions with a client.
- **Stateless:**Â Each request is independent, and the server does not store session data.

**Example:**

- **Stateful:**Â Online gaming sessions (game state is stored).
- **Stateless:**Â REST APIs (each request carries all necessary information).

---

### **Strong Consistency vs. Eventual Consistency**

- Strong Consistency:Â Every read gets the latest data immediately.
- **Eventual Consistency:**Â Reads might return old data, but all nodes will eventually synchronize.

**Example:**

- **Strong Consistency:**Â Banking systems (money transfers must be consistent).
- **Eventual Consistency:**Â Social media feeds (you might see an old post before updates arrive).

---

### **Read-Through vs. Write-Through Cache**

- Read-Through Cache:Â If data is not in the cache, it is fetched from the database and stored in the cache.
- **Write-Through Cache:**Â Every write operation isÂ **simultaneously**Â updated in the cache and database.

**Example:**

- **Read-Through:**Â Redis caching website products (if not in cache, fetch from DB).
- **Write-Through:**Â Updates in the user profile instantly reflect in cache & DB.

---

### **Push vs. Pull Architecture**

- Push:Â The serverÂ sends updatesÂ to clients as soon as they happen.
- **Pull:**Â ClientsÂ **request updates**Â at intervals.

**Example:**

- **Push:**Â Live cricket score notifications.
- **Pull:**Â Checking emails manually.

---

### **REST vs. RPC**

- REST (Representational State Transfer):Â Uses HTTP methods (GET,Â POST) and isÂ stateless.
- **RPC (Remote Procedure Call):**Â Calls functions on a remote serverÂ **directly**.

**Example:**

- **REST:**Â `GET /users/123`Â (fetch user info).
- **RPC:**Â `getUser(123)`Â (direct function call).

---

### **Synchronous vs. Asynchronous Communication**

- Synchronous:Â A request waits for a response before continuing.
- **Asynchronous:**Â A request continues executing without waiting.

**Example:**

- **Synchronous:**Â Calling an API and waiting for a response (`fetch()`Â withoutÂ `async/await`).
- **Asynchronous:**Â Sending an email (does not block execution).

---

### **Latency vs. Throughput**

- Latency:Â Time taken toÂ completeÂ a single request.
- **Throughput:**Â Number of requests processedÂ **per second**.

**Example:**

- **Low latency:**Â Google Search (results appear instantly).
- **High throughput:**Â Netflix handling millions of streams at once.